<p align="center">
  <a href="https://github.com/txt/guru26spr/blob/main/README.md"><img 
     src="https://img.shields.io/badge/Home-%23ff5733?style=flat-square&logo=home&logoColor=white" /></a>
  <a href="https://github.com/txt/guru26spr/blob/main/docs/lect/syllabus.md"><img 
      src="https://img.shields.io/badge/Syllabus-%230055ff?style=flat-square&logo=openai&logoColor=white" /></a>
  <a href="https://docs.google.com/spreadsheets/d/1xZfIwkmu6hTJjXico1zIzklt1Tl9-L9j9uHrix9KToU/edit?usp=sharing"><img
      src="https://img.shields.io/badge/Teams-%23ffd700?style=flat-square&logo=users&logoColor=white" /></a>
  <a href="https://moodle-courses2527.wolfware.ncsu.edu/course/view.php?id=8119"><img 
      src="https://img.shields.io/badge/Moodle-%23dc143c?style=flat-square&logo=moodle&logoColor=white" /></a>
  <a href="https://discord.gg/vCCXMfzQ"><img 
      src="https://img.shields.io/badge/Chat-%23008080?style=flat-square&logo=discord&logoColor=white" /></a>
  <a href="https://github.com/txt/guru26spr/blob/main/LICENSE.md"><img 
      src="https://img.shields.io/badge/©%20timm%202026-%234b4b4b?style=flat-square&logoColor=white" /></a></p>
<h1 align="center">:cyclone: CSC491/591: How to be a SE Guru <br>NC State, Spring '26</h1>
<img src="https://raw.githubusercontent.com/txt/guru26spr/refs/heads/main/etc/img/banenr.png"> 

## Introduction: The View from 2036

* **A Decade of Radical Change:** Software Engineering (SE) publishing has evolved fundamentally since the mid-2020s.
* **The Core Shift:** We moved from isolated, gatekeeping journals to a unified, community-driven evaluation ecosystem.
* **Today's Focus:** Looking back at the growing pains of the 2020s and how we built the collaborative infrastructure we rely on today.

> "There is wealth not only in the final paper but also in the discussions that led to its acceptance."

## The "Dark Ages" (Pre-2026)

* **Extreme Fragmentation:** Venues operated as completely independent entities.
* **Opaque Processes:** Anonymous reviews were discarded after rejection; historical reviewer data was lost.
* **Redundant Workloads:** Reviewers evaluated "re-submitted" papers from scratch, wasting thousands of hours.
* **Siloed Management:** Program Chairs and Editors-in-Chief had no visibility into a manuscript's past journey across the field.

## Symptoms of a Failing System

* **Reviewer Burnout:** The same limited pool of experts was tapped repeatedly across disconnected venues.
* **Inefficiency:** Double submissions and plagiarism were notoriously difficult to catch.
* **Loss of Scientific Context:** The rich debate between authors and reviewers vanished the moment a paper was rejected.

> "Each of these crucial resources were tied to specific venues and tasked with paper evaluation workloads as if other venues were non-existent."

## The Tipping Point & The eLife Inspiration

* **The Breaking Point:** Exponential submission growth made the traditional binary "Accept/Reject" model mathematically unsustainable.
* **A New Model Emerges:** Inspired by pioneers like *eLife*, the SE community realized we had to decouple **scientific evaluation** from **venue publication**.
* **The Goal:** Assess the science first, decide on the "home" for the paper second.

## The Paradigm Shift: Collaborative Infrastructure

* **Breaking Down Silos:** Venues agreed to share procedures, infrastructure, and historical data.
* **Persistent Reviewer Profiles:** Reviewers, board members, and editors became field-wide resources, not just venue-specific labor.
* **Unified Backends:** Manuscripts carry their history—reviews, revisions, and editor notes—from submission to final publication.

## The Rise of "Human + AI" Review

* **Augmented Intelligence:** AI transitioned from a novelty to an essential partner in peer review.
* **Division of Labor:** * **AI:** Handles structural checks, statistical validation, and methodological completeness.
  * **Humans:** Focus on novelty, impact, nuance, and complex architectural reasoning.
* **Accelerated Science:** Human+AI synergy drastically reduced time-to-decision without sacrificing rigor.

## Transparency and The Living Manuscript

* **The End of Static PDFs:** Accepted papers now showcase their evolutionary journey.
* **Visible Trajectories:** Readers can see exactly how authors improved a paper based on initial critiques.
* **Meta-Reviews:** Subsequent reviewers can see and react to the opinions of previous reviewers, creating a continuous thread of scientific dialogue.

## Venues as Curators, Not Gatekeepers

* **The "Digest" Model:** Venues transformed from pure evaluation-machines into curated digests.
* **Editorial Vision:** Journals and conferences now select papers from a pre-evaluated, community-wide pool.
* **Diverse Perspectives:** Venues highlight specific viewpoints within the SE field rather than acting as a simple filter for "correctness."

## The Wealth of Dialogue

* **Beyond the Final Text:** We recognized that the review process *itself* is a core scientific contribution.
* **Citable Reviews:** Insightful reviews and constructive dialogues are now recognized, rewarded, and cited alongside the primary research.
* **Community Effort:** Evaluation is now acknowledged as a unified, field-wide endeavor rather than a solitary, hidden task.

## Looking Forward: SE Research Post-2036

* **Continuous Evolution:** Our shared infrastructure continues to adapt to new methodologies.
* **Global Equity:** Open data and shared review histories have leveled the playing field for emerging institutions.
* **The Final Takeaway:** Collaboration, transparency, and augmented intelligence saved SE publishing from collapse.

## References

* **[manrai2025accelerating]:** Arjun K. Manrai *et al.*, "Accelerating science with Human+ AI review," *NEJM AI*, 2025.
* **[moot2026benchmarks]:** Tim Menzies *et al.*, "MOOT: a Repository of Many Multi-Objective Optimization Tasks," *2026 IEEE/ACM 23rd International Conference on Mining Software Repositories (MSR)*, 2026.
* **[eLife202X]:** eLife *et al.*, "What happened when eLife decided to eliminate accept/reject decisions after peer review?," *eLife*, n.d.
* **[wohlin2012experimentation]:** Claes Wohlin *et al.*, "Experimentation in software engineering," *Springer*, 2012.
* **[runeson2009guidelines]:** Per Runeson *et al.*, "Guidelines for conducting and reporting case study research in software engineering," *Empirical Software Engineering*, 2009.
* **[theisen2017writing]:** Christopher Theisen *et al.*, "Writing good software engineering research papers: Revisited," *ICSE-C*, 2017.
