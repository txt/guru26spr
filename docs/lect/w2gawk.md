.

# gawk for newbies (as used in `ell` and the Makefile)

Just enough gawk to understand *all* the awk in `ell` and the Makefile.

---

## Why awk wins: A real example

**nbc.awk** ‚Äî a Naive Bayes classifier in ~100 lines ‚Äî matches or beats WEKA's industrial-strength Java implementation:

 
Accuracy comparison (15 datasets, 10-fold cross-val):
- Equal or better on 11/15 datasets
- Statistically significant wins on 3 datasets (breast-w, credit-a, letter)
- Only 1 significant loss (soybean)

Runtime comparison:
- Faster on 10/15 datasets (all datasets <1000 instances)
- For small data: 2-5x faster than Java
- For large data: ~1.4x slower overall
- Zero dependencies, zero installation
 

This is the awk value proposition: **20 lines of portable code vs megabytes of enterprise framework**, with competitive performance where it matters (small-to-medium data, rapid iteration, teaching, prototyping).

---

## Word frequency in 2 lines

```sh
gawk '{ for (i=1;i<=NF;i++) Count[$i]++ }
      END { for (w in Count) print w, Count[w] }' file.txt
```

No imports. No declarations. Hash maps on demand. Streaming. This is the awk mindset.

---

## The 20 essentials

**1. Program structure**
```awk
pattern { action }
```
Pattern true ‚Üí action runs. No pattern ‚Üí always. No action ‚Üí print.

Hemce the workd's simplexisst progrm to print very line:

```
1
```

**2. Records and fields**
```awk
$0         # entire line
$1, $2     # first, second field
NF         # number of fields
NR         # line number
$NF        # last field
```

**3. Separators**
```awk
BEGIN { FS=","; RS="" }  # CSV fields, paragraph records
```

**4. BEGIN and END**
```awk
BEGIN { print "Starting" }      # before any input
      { sum += $1 }              # per record
END   { print "Total:", sum }   # after all input
```

**5. Variables materialize**
```awk
count++           # starts at 0
name = name $1    # starts at ""
```
No declarations. Ever.

**6. Associative arrays (the killer feature)**
```awk
Freq[$1]++                    # count occurrences
Seen[class, i, $i] = 1        # multi-key lookup
for (k in Freq) print k       # iterate
```
Hash maps everywhere. This is why awk punches above its weight.

**7. Multi-dimensional keys**
```awk
Data[row, col, val]++
```
Internally one string key, but awk treats them as real dimensions.

**8. Functions**
```awk
function dist(a,b,   i, d) {  # args, then locals (extra spaces)
  for (i in a) d += (a[i]-b[i])^2
  return sqrt(d)
}
```

**9. Control flow**
```awk
if (x > 0) ...
for (i=1; i<=NF; i++) ...
for (k in array) ...
while (x > 0) ...
```

**10. next**
```awk
NR<=10 { train(); next }  # skip rest of rules, go to next record
{ test() }                 # only runs if NR > 10
```

**11. Built-in math**
```awk
log($1), exp($2), sqrt($3), int($4)
```
No imports.

**12. Random numbers**
```awk
BEGIN { srand(Seed) }
{ if (rand() < 0.1) print }  # 10% sample
```

**13. Command-line variables**
```sh
gawk -v Seed=$RANDOM -v K=5 'BEGIN { srand(Seed); print K }'
```
Essential for parameterized scripts in Makefiles.

**14. Pattern expressions**
```awk
NR==1                     # header line
NR>1 && $3 > 100         # data lines with condition
/^#/                      # regex match
```

**15. Printing**
```awk
print                     # prints $0
print $1, $2             # comma ‚Üí OFS (default space)
print $1 "," $2          # string concatenation
```

**16. String operations**
```awk
s = a b              # concatenation (adjacency)
s = a "," b          # explicit separator
split($0, arr, ",")  # split into array
sub(/old/, "new")    # replace first
gsub(/old/, "new")   # replace all
```

**17. Numeric vs string**
```awk
x + 0       # force numeric
x ""        # force string
```
Context determines conversion.

**18. Arrays are references**
```awk
function modify(arr) { arr[1] = 99 }
modify(data)  # data[1] is now 99
```

**19. Pipelines**
```awk
sort | uniq | gawk '{sum+=$1} END {print sum}'
```
awk is built for Unix composition.

**20. It's a real language**
```awk
function knn(data, k,   dists, i, n) {
  for (i in data) {
    dists[++n] = distance(query, data[i]) SUBSEP i
  }
  asort(dists)
  # ... voting logic
}
```
Functions, state, probabilities, learning. Not just regex glue.

---

## Mental model (important)

**awk is a streaming interpreter with associative memory**

Once that clicks, the gawk in `ell` and the Makefile stops looking clever and starts looking inevitable.

---

## Why this matters for `ell` and research prototyping

- **Zero dependencies**: runs everywhere Unix runs
- **Tiny code**: easier to understand, modify, teach
- **Fast iteration**: edit ‚Üí test in seconds
- **Competitive performance**: nbc.awk proves small can match big
- **Reproducible**: no version conflicts, no library updates breaking code
- **Transparent**: all logic visible in ~100 lines

This is "simple ain't stupid" in action.
#
## Example 1: Random Sig Generation

 
## Example: understanding `sigs` (short)

### Goal
From a file of **paragraphs**, print:
1. the header (optional)
2. **one random quote**

---

### Example input (from the quotes file)

```
Instant gratification takes too long.
-- Carrie Fisher

Complexity is easy. Simplicity is hard.
--Unknown

Motto for a research laboratory: What we work on today, others will
first think of tomorrow.  -- Alan Perlis

Every program is a part of some other program and rarely fits.
-- Alan Perlis

One man's constant is another man's variable.
-- Alan Perlis

````

Each quote is a *paragraph* (blank lines separate them).

The awk core

```sh
gawk -v Seed="$RANDOM" \
'BEGIN           { srand(Seed); RS="" }
                 { Recs[rand()] = $0 }
 END             { for (R in Recs) {print Recs[R]; exit} }
' quotes.txt
````


How it works

* `RS=""`
  ‚Üí read **paragraphs**, not lines

* `Recs[rand()] = $0`
  ‚Üí store each quote under a random key

* `for (R in Recs)`
  ‚Üí hash iteration order is random

* `exit`
  ‚Üí print **one** quote and stop

Note:

* No temp files
* No sorting
* No lists
* Pure streaming
* Random sampling in ~5 lines

Mental model

- **awk = stream + hash tables + patterns**
- That‚Äôs all `sigs` is doing.

## Example2: Highlight columns


The shell function:

```sh
hi() { # usage: command | hi 7 10 12
  gawk -v cols="$*" '
    BEGIN { split(cols, tmp, " ");
            for (i in tmp) lite[tmp[i]] = i + 30 }
          { for (i in lite) {
              col = i < 0 ? NF + i + 1 : i
              $col = "\033[1;"lite[i]"m" $col "\033[0m" }
            print $0; }'
}
````

Example input

```sh
printf "%s\n" \
"-rw-r--r-- 1 tim staff 4096 Jan 21 notes.txt" \
"drwxr-xr-x 3 tim staff  96 Jan 20 src"
```


Pipe to `hi`

```sh
printf "%s\n" \
"-rw-r--r-- 1 tim staff 4096 Jan 21 notes.txt" \
"drwxr-xr-x 3 tim staff  96 Jan 20 src" | hi 5
```

Sample output (with ANSI colors):

```
-rw-r--r-- 1 tim staff [1;31m4096[0m Jan 21 notes.txt
drwxr-xr-x 3 tim staff [1;31m96[0m Jan 20 src
```

(Here column **5** is printed in **bold red**.)


Another example (negative index)

```sh
ls -l | hi -1
```

Highlights the **last column** (filename), regardless of width.


What this shows:

* `\033[1;31m` ‚Üí start bold red
* `\033[0m` ‚Üí reset color
* awk edits fields *in place*
* works cleanly in Unix pipes

Takeaway

- **`hi` is a field-aware, stream-based highlighter built in ~10 lines of awk.**

## Example3:

nbc.awk ‚Äî Naive Bayes Classifier in 18 Lines

A streaming Naive Bayes classifier that trains on the first 10 lines, then classifies and learns from each subsequent line.

---

### Sample Input

```
sunny,hot,high,weak,no
sunny,hot,high,strong,no
overcast,hot,high,weak,yes
rainy,mild,high,weak,yes
rainy,cool,normal,weak,yes
rainy,cool,normal,strong,no
overcast,cool,normal,strong,yes
sunny,mild,high,weak,no
sunny,cool,normal,weak,yes
rainy,mild,normal,weak,yes
sunny,mild,normal,strong,yes
overcast,mild,high,strong,yes
overcast,hot,normal,weak,yes
rainy,mild,high,strong,no
```

Last column is class (`yes`/`no` for "play tennis?"). First 10 lines = training. Lines 11-14 = test.

---

### Sample Output

```
yes,yes
yes,yes
yes,yes
no,no
```

Format: `actual,predicted`

Lines 11-14 classified correctly (100% accuracy on this toy example).

---

### Code Walkthrough

#### Setup and flow control

```awk
BEGIN {Total=0}
```
Initialize counter for total training instances.

```awk
NR<=10 {train();next}
```
Lines 1-10: train only, skip classification.

```awk
       {c=classify();print $NF","c;train()}
```
Lines 11+: classify current line, print `actual,predicted`, then train on it (incremental learning).

---

#### Function: train()

```awk
function train(    i,c) { 
  Total++;c=$NF;Classes[c]++;
```
- Increment total instance count
- Extract class from last field (`$NF`)
- Increment count for this class

```awk
  for(i=1;i<=NF;i++){
    if($i=="?")continue;
```
- Loop through all fields (including class)
- Skip missing values (`?`)

```awk
    Freq[c,i,$i]++;
```
- **Key line**: Count how often value `$i` appears in column `i` for class `c`
- Multi-dimensional hash: `Freq[yes,1,sunny]++` means "sunny in column 1 appeared with class yes"

```awk
    if(++Seen[i,$i]==1)Attributes[i]++}
}
```
- Track unique values per attribute for Laplace smoothing
- `Attributes[i]` = number of distinct values in column `i`

---

#### Function: classify()

```awk
function classify(    i,t,w,like,c) {  
  like=-1e9;
```
- `like` = best log-likelihood so far (start at negative infinity)

```awk
  for(c in Classes){  
    t=log(Classes[c]/Total);
```
- For each known class, start with prior: `P(c) = Classes[c] / Total`
- Use logs to avoid underflow

```awk
    for(i=1;i<NF;i++){  
      if($i=="?")continue;
      t+=log((Freq[c,i,$i]+1)/(Classes[c]+Attributes[i]))};
```
- **Naive Bayes formula**: multiply conditional probabilities
- `P(feature|class)` = `(Freq[c,i,$i] + 1) / (Classes[c] + Attributes[i])`
- `+1` numerator and `+Attributes[i]` denominator = Laplace smoothing
- Loop stops at `NF-1` to exclude the class label

```awk
    if(t>=like){like=t;w=c}};
  return w;
}
```
- Track class with highest log-likelihood
- Return predicted class

---

### Key Data Structures

```awk
Classes[c]           # Count of instances per class
                     # Classes["yes"] = 7, Classes["no"] = 3

Freq[c,i,val]        # Count of value in column i for class c
                     # Freq["yes",1,"sunny"] = 2

Seen[i,val]          # Has this value been seen in column i?
                     # Seen[1,"sunny"] = 1 (boolean)

Attributes[i]        # Number of distinct values in column i
                     # Attributes[1] = 3 (sunny, overcast, rainy)
```

---

### Why This Works

**Naive Bayes assumption**: features are independent given the class.

**Formula**:
```
P(class|features) ‚àù P(class) √ó P(f‚ÇÅ|class) √ó P(f‚ÇÇ|class) √ó ...
```

**In logs** (to avoid underflow):
```
log P(class|features) = log P(class) + Œ£ log P(f·µ¢|class)
```

**Laplace smoothing**: Add 1 to numerator, add count of possible values to denominator.  
Prevents zero probabilities for unseen feature values.

---

### Running It

```sh
gawk -F, -f nbc.awk data.csv
```

- `-F,` sets field separator to comma
- First 10 lines train the model silently
- Lines 11+ print `actual,predicted` and update the model

---

### Extensions in Full nbc.awk

The 18-line core above handles:
- Training
- Classification  
- Incremental learning

Full nbc.awk (~100 lines) adds:
- Cross-validation
- Missing value handling
- Continuous attribute discretization
- Evaluation metrics

But the core logic? 18 lines. Zero dependencies.

